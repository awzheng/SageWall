{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd8713-3ec7-4819-bdd9-154b8dca9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# 1. Setup Environment\n",
    "# -------------------\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = session.boto_region_name\n",
    "bucket_processed = 'sagewall-processed-zheng-1b'  # Your processed bucket\n",
    "prefix = 'model-input'\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "# 2. Prepare Data (Split into Train/Validation)\n",
    "# ---------------------------------------------\n",
    "print(\"\\nDownloading data from S3...\")\n",
    "s3 = boto3.client('s3')\n",
    "# Download the clean file we made in Phase 2\n",
    "s3.download_file(bucket_processed, 'KDDTrain+.txt', 'clean_data.csv')\n",
    "\n",
    "# Read with Pandas (No headers, as per our Lambda format)\n",
    "df = pd.read_csv('clean_data.csv', header=None)\n",
    "\n",
    "# Split: 80% for Training, 20% for Validation (The \"Exam\")\n",
    "train_data = df.sample(frac=0.8, random_state=42)\n",
    "val_data = df.drop(train_data.index)\n",
    "\n",
    "# Save locally without headers (SageMaker requirement)\n",
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "val_data.to_csv('validation.csv', index=False, header=False)\n",
    "\n",
    "# Upload the split files back to S3\n",
    "print(\"Uploading split datasets to S3...\")\n",
    "session.upload_data('train.csv', bucket=bucket_processed, key_prefix=prefix)\n",
    "session.upload_data('validation.csv', bucket=bucket_processed, key_prefix=prefix)\n",
    "\n",
    "s3_train_input = TrainingInput(s3_data=f's3://{bucket_processed}/{prefix}/train.csv', content_type='csv')\n",
    "s3_val_input = TrainingInput(s3_data=f's3://{bucket_processed}/{prefix}/validation.csv', content_type='csv')\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "# 3. Define the XGBoost Model\n",
    "# ---------------------------\n",
    "# We retrieve the official XGBoost Docker image from AWS\n",
    "container = image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "\n",
    "# Create the Estimator (The \"Teacher\")\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',  # Standard training instance\n",
    "    output_path=f's3://{bucket_processed}/model-output/',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Set Hyperparameters (The \"Teaching Style\")\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    objective='binary:logistic',  # \"Is it an Attack? Yes/No\"\n",
    "    num_round=50\n",
    ")\n",
    "\n",
    "# 4. Start Training\n",
    "# -----------------\n",
    "print(\"\\nStarting Training Job... (This takes ~3-5 minutes)\")\n",
    "xgb_estimator.fit({'train': s3_train_input, 'validation': s3_val_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f16f7-81b8-4e59-b0ba-cc7cb6017467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
