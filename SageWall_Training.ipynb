{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abe94da",
   "metadata": {},
   "source": [
    "# ðŸ›¡ï¸ SageWall: XGBoost Training Pipeline\n",
    "\n",
    "**Author:** Andrew Zheng | **Date:** December 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook trains and deploys a **binary classification model** to detect network intrusions in real-time. We use **XGBoost** (eXtreme Gradient Boosting) on AWS SageMaker to analyze network packet features and classify traffic as either **Normal** or **Attack**.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| **1. Setup** | Initialize AWS SDKs and configure IAM permissions |\n",
    "| **2. Data Preparation** | Split dataset into training (80%) and validation (20%) |\n",
    "| **3. Model Training** | Configure XGBoost hyperparameters and train on SageMaker |\n",
    "| **4. Deployment** | Provision a real-time inference endpoint |\n",
    "| **5. Testing** | Validate model accuracy on held-out samples |\n",
    "| **6. Cleanup** | Delete resources to stop billing |\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- AWS credentials configured (`aws configure`)\n",
    "- Processed data in S3 bucket (`sagewall-processed-zheng-1b`)\n",
    "- SageMaker execution role with appropriate permissions\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa95ec",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Data Preparation\n",
    "\n",
    "### 1.1 Initializing AWS SDKs\n",
    "\n",
    "We begin by importing the necessary libraries and establishing connections to AWS services:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|---------|\n",
    "| `sagemaker` | High-level SDK for ML training and deployment |\n",
    "| `boto3` | Low-level AWS SDK for S3 operations |\n",
    "| `pandas` | Data manipulation and CSV handling |\n",
    "\n",
    "**IAM Role:** SageMaker requires an execution role with permissions to:\n",
    "- Read/write to S3 buckets\n",
    "- Create training jobs and endpoints\n",
    "- Write logs to CloudWatch\n",
    "\n",
    "The `get_execution_role()` function automatically retrieves this role when running in SageMaker Studio.\n",
    "\n",
    "### 1.2 The 80/20 Split Strategy\n",
    "\n",
    "Machine learning models need two datasets:\n",
    "\n",
    "1. **Training Set (80%)** â€” The model learns patterns from this data\n",
    "2. **Validation Set (20%)** â€” We test accuracy on data the model has *never seen*\n",
    "\n",
    "> âš ï¸ **Why split?** If we test on training data, we get falsely high accuracy. The model could simply \"memorize\" answers instead of learning patterns.\n",
    "\n",
    "We use `random_state=42` to ensure reproducibility â€” running the code again produces identical splits.\n",
    "\n",
    "### 1.3 XGBoost Configuration\n",
    "\n",
    "**Why XGBoost?** It's consistently the top-performing algorithm on tabular data competitions. Key hyperparameters:\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|-----------|-------|-------------|\n",
    "| `max_depth` | 5 | Tree depth â€” prevents overfitting (too complex = memorization) |\n",
    "| `eta` | 0.2 | Learning rate â€” smaller = more cautious learning |\n",
    "| `gamma` | 4 | Minimum loss reduction â€” regularization to prevent overfitting |\n",
    "| `min_child_weight` | 6 | Minimum samples per leaf â€” avoids learning from noise |\n",
    "| `subsample` | 0.8 | Use 80% of data per tree â€” reduces variance |\n",
    "| `objective` | `binary:logistic` | Binary classification (Attack vs Normal) |\n",
    "| `num_round` | 50 | Number of boosting rounds (trees to build) |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dd8713-3ec7-4819-bdd9-154b8dca9005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Role: arn:aws:iam::015283627438:role/service-role/AmazonSageMaker-ExecutionRole-20251221T121867\n",
      "\n",
      "Downloading data from S3...\n",
      "Uploading split datasets to S3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-12-21-17-36-28-303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete.\n",
      "\n",
      "Starting Training Job... (This takes ~3-5 minutes)\n",
      "2025-12-21 17:36:29 Starting - Starting the training job...\n",
      "2025-12-21 17:36:44 Starting - Preparing the instances for training...\n",
      "2025-12-21 17:37:06 Downloading - Downloading input data...\n",
      "2025-12-21 17:37:56 Downloading - Downloading the training image......\n",
      "2025-12-21 17:38:47 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-12-21 17:38:50.098 ip-10-0-205-127.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-12-21 17:38:50.119 ip-10-0-205-127.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:50:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:50:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:50:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:50:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:50:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:50:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:51:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:51:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:51:INFO] Train matrix has 100778 rows and 122 columns\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:51:INFO] Validation matrix has 25195 rows\u001b[0m\n",
      "\u001b[34m[2025-12-21 17:38:51.521 ip-10-0-205-127.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-12-21 17:38:51.522 ip-10-0-205-127.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-12-21 17:38:51.522 ip-10-0-205-127.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-12-21 17:38:51.522 ip-10-0-205-127.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-12-21:17:38:51:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[17:38:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\u001b[34m[2025-12-21 17:38:52.389 ip-10-0-205-127.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-12-21 17:38:52.394 ip-10-0-205-127.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-logloss:0.52014#011validation-logloss:0.52052\u001b[0m\n",
      "\u001b[34m[1]#011train-logloss:0.40422#011validation-logloss:0.40469\u001b[0m\n",
      "\u001b[34m[2]#011train-logloss:0.32097#011validation-logloss:0.32155\u001b[0m\n",
      "\u001b[34m[3]#011train-logloss:0.25883#011validation-logloss:0.25950\u001b[0m\n",
      "\u001b[34m[4]#011train-logloss:0.21081#011validation-logloss:0.21146\u001b[0m\n",
      "\u001b[34m[5]#011train-logloss:0.17427#011validation-logloss:0.17488\u001b[0m\n",
      "\u001b[34m[6]#011train-logloss:0.14482#011validation-logloss:0.14535\u001b[0m\n",
      "\u001b[34m[7]#011train-logloss:0.12001#011validation-logloss:0.12047\u001b[0m\n",
      "\u001b[34m[8]#011train-logloss:0.10027#011validation-logloss:0.10052\u001b[0m\n",
      "\u001b[34m[9]#011train-logloss:0.08372#011validation-logloss:0.08392\u001b[0m\n",
      "\u001b[34m[10]#011train-logloss:0.07088#011validation-logloss:0.07104\u001b[0m\n",
      "\u001b[34m[11]#011train-logloss:0.06032#011validation-logloss:0.06040\u001b[0m\n",
      "\u001b[34m[12]#011train-logloss:0.05181#011validation-logloss:0.05194\u001b[0m\n",
      "\u001b[34m[13]#011train-logloss:0.04491#011validation-logloss:0.04492\u001b[0m\n",
      "\u001b[34m[14]#011train-logloss:0.03827#011validation-logloss:0.03838\u001b[0m\n",
      "\u001b[34m[15]#011train-logloss:0.03323#011validation-logloss:0.03322\u001b[0m\n",
      "\u001b[34m[16]#011train-logloss:0.02901#011validation-logloss:0.02907\u001b[0m\n",
      "\u001b[34m[17]#011train-logloss:0.02555#011validation-logloss:0.02559\u001b[0m\n",
      "\u001b[34m[18]#011train-logloss:0.02271#011validation-logloss:0.02277\u001b[0m\n",
      "\u001b[34m[19]#011train-logloss:0.02051#011validation-logloss:0.02062\u001b[0m\n",
      "\u001b[34m[20]#011train-logloss:0.01877#011validation-logloss:0.01885\u001b[0m\n",
      "\u001b[34m[21]#011train-logloss:0.01715#011validation-logloss:0.01722\u001b[0m\n",
      "\u001b[34m[22]#011train-logloss:0.01596#011validation-logloss:0.01601\u001b[0m\n",
      "\u001b[34m[23]#011train-logloss:0.01495#011validation-logloss:0.01500\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.01424#011validation-logloss:0.01429\u001b[0m\n",
      "\u001b[34m[25]#011train-logloss:0.01351#011validation-logloss:0.01353\u001b[0m\n",
      "\u001b[34m[26]#011train-logloss:0.01299#011validation-logloss:0.01304\u001b[0m\n",
      "\u001b[34m[27]#011train-logloss:0.01256#011validation-logloss:0.01260\u001b[0m\n",
      "\u001b[34m[28]#011train-logloss:0.01210#011validation-logloss:0.01217\u001b[0m\n",
      "\u001b[34m[29]#011train-logloss:0.01173#011validation-logloss:0.01174\u001b[0m\n",
      "\u001b[34m[30]#011train-logloss:0.01124#011validation-logloss:0.01121\u001b[0m\n",
      "\u001b[34m[31]#011train-logloss:0.01069#011validation-logloss:0.01073\u001b[0m\n",
      "\u001b[34m[32]#011train-logloss:0.01049#011validation-logloss:0.01052\u001b[0m\n",
      "\u001b[34m[33]#011train-logloss:0.01009#011validation-logloss:0.01014\u001b[0m\n",
      "\u001b[34m[34]#011train-logloss:0.00987#011validation-logloss:0.00994\u001b[0m\n",
      "\u001b[34m[35]#011train-logloss:0.00940#011validation-logloss:0.00956\u001b[0m\n",
      "\u001b[34m[36]#011train-logloss:0.00910#011validation-logloss:0.00927\u001b[0m\n",
      "\u001b[34m[37]#011train-logloss:0.00877#011validation-logloss:0.00895\u001b[0m\n",
      "\u001b[34m[38]#011train-logloss:0.00858#011validation-logloss:0.00880\u001b[0m\n",
      "\u001b[34m[39]#011train-logloss:0.00824#011validation-logloss:0.00846\u001b[0m\n",
      "\u001b[34m[40]#011train-logloss:0.00810#011validation-logloss:0.00833\u001b[0m\n",
      "\u001b[34m[41]#011train-logloss:0.00800#011validation-logloss:0.00824\u001b[0m\n",
      "\u001b[34m[42]#011train-logloss:0.00775#011validation-logloss:0.00798\u001b[0m\n",
      "\u001b[34m[43]#011train-logloss:0.00729#011validation-logloss:0.00755\u001b[0m\n",
      "\u001b[34m[44]#011train-logloss:0.00716#011validation-logloss:0.00743\u001b[0m\n",
      "\u001b[34m[45]#011train-logloss:0.00698#011validation-logloss:0.00726\u001b[0m\n",
      "\u001b[34m[46]#011train-logloss:0.00689#011validation-logloss:0.00718\u001b[0m\n",
      "\u001b[34m[47]#011train-logloss:0.00663#011validation-logloss:0.00696\u001b[0m\n",
      "\u001b[34m[48]#011train-logloss:0.00654#011validation-logloss:0.00685\u001b[0m\n",
      "\u001b[34m[49]#011train-logloss:0.00627#011validation-logloss:0.00664\u001b[0m\n",
      "\n",
      "2025-12-21 17:39:31 Uploading - Uploading generated training model\n",
      "2025-12-21 17:39:31 Completed - Training job completed\n",
      "Training seconds: 145\n",
      "Billable seconds: 145\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# 1. Setup Environment\n",
    "# -------------------\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = session.boto_region_name\n",
    "bucket_processed = 'sagewall-processed-zheng-1b'  # Your processed bucket\n",
    "prefix = 'model-input'\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "# 2. Prepare Data (Split into Train/Validation)\n",
    "# ---------------------------------------------\n",
    "print(\"\\nDownloading data from S3...\")\n",
    "s3 = boto3.client('s3')\n",
    "# Download the clean file we made in Phase 2\n",
    "s3.download_file(bucket_processed, 'KDDTrain+.txt', 'clean_data.csv')\n",
    "\n",
    "# Read with Pandas (No headers, as per our Lambda format)\n",
    "df = pd.read_csv('clean_data.csv', header=None)\n",
    "\n",
    "# Split: 80% for Training, 20% for Validation (The \"Exam\")\n",
    "train_data = df.sample(frac=0.8, random_state=42)\n",
    "val_data = df.drop(train_data.index)\n",
    "\n",
    "# Save locally without headers (SageMaker requirement)\n",
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "val_data.to_csv('validation.csv', index=False, header=False)\n",
    "\n",
    "# Upload the split files back to S3\n",
    "print(\"Uploading split datasets to S3...\")\n",
    "session.upload_data('train.csv', bucket=bucket_processed, key_prefix=prefix)\n",
    "session.upload_data('validation.csv', bucket=bucket_processed, key_prefix=prefix)\n",
    "\n",
    "s3_train_input = TrainingInput(s3_data=f's3://{bucket_processed}/{prefix}/train.csv', content_type='csv')\n",
    "s3_val_input = TrainingInput(s3_data=f's3://{bucket_processed}/{prefix}/validation.csv', content_type='csv')\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "# 3. Define the XGBoost Model\n",
    "# ---------------------------\n",
    "# We retrieve the official XGBoost Docker image from AWS\n",
    "container = image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "\n",
    "# Create the Estimator (The \"Teacher\")\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',  # Standard training instance\n",
    "    output_path=f's3://{bucket_processed}/model-output/',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Set Hyperparameters (The \"Teaching Style\")\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    objective='binary:logistic',  # \"Is it an Attack? Yes/No\"\n",
    "    num_round=50\n",
    ")\n",
    "\n",
    "# 4. Start Training\n",
    "# -----------------\n",
    "print(\"\\nStarting Training Job... (This takes ~3-5 minutes)\")\n",
    "xgb_estimator.fit({'train': s3_train_input, 'validation': s3_val_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93761f16",
   "metadata": {},
   "source": [
    "## 2. Model Deployment\n",
    "\n",
    "### Creating a Real-Time Inference Endpoint\n",
    "\n",
    "After training completes, the model artifact (a serialized XGBoost file) is saved to S3. To use it for predictions, we must **deploy** it to a live endpoint.\n",
    "\n",
    "**What happens during deployment:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Model Artifact â”‚  â†’   â”‚  Docker         â”‚  â†’   â”‚  EC2 Instance   â”‚\n",
    "â”‚  (S3 .tar.gz)   â”‚      â”‚  Container      â”‚      â”‚  (ml.t2.medium) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "1. SageMaker downloads the model from S3\n",
    "2. Loads it into a pre-built XGBoost Docker container\n",
    "3. Provisions an EC2 instance to host the API\n",
    "4. Exposes an HTTPS endpoint for real-time predictions\n",
    "\n",
    "### Instance Selection\n",
    "\n",
    "| Instance | vCPU | Memory | Cost/Hour | Use Case |\n",
    "|----------|------|--------|-----------|----------|\n",
    "| `ml.t2.medium` | 2 | 4 GB | ~$0.05 | Development/Testing âœ“ |\n",
    "| `ml.m5.large` | 2 | 8 GB | ~$0.12 | Light Production |\n",
    "| `ml.c5.xlarge` | 4 | 8 GB | ~$0.20 | High Throughput |\n",
    "\n",
    "We use `ml.t2.medium` for cost-effective testing. For production workloads with high request volume, scale up to `ml.m5` or `ml.c5` instances.\n",
    "\n",
    "> â±ï¸ **Deployment takes 5-8 minutes** â€” SageMaker must provision infrastructure and health-check the endpoint.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b388b7ee-595f-4cb2-b417-c5015bfe2822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-12-21-17-42-13-588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to a real-time endpoint... (This takes 5-8 minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2025-12-21-17-42-13-588\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2025-12-21-17-42-13-588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!\n",
      "Model deployed! Endpoint name: sagemaker-xgboost-2025-12-21-17-42-13-588\n"
     ]
    }
   ],
   "source": [
    "# 5. Deploy the Model\n",
    "# -------------------\n",
    "print(\"Deploying model to a real-time endpoint... (This takes 5-8 minutes)\")\n",
    "\n",
    "# We create a \"Predictor\" (The Interface)\n",
    "xgb_predictor = xgb_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',  # Cost-effective instance for inference\n",
    "    serializer=sagemaker.serializers.CSVSerializer() # We will send it CSV data\n",
    ")\n",
    "\n",
    "print(f\"\\nModel deployed! Endpoint name: {xgb_predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe3f9f",
   "metadata": {},
   "source": [
    "## 3. Model Testing & Validation\n",
    "\n",
    "### Evaluating on Held-Out Data\n",
    "\n",
    "Now we test the deployed model on our **validation set** â€” data the model has never seen during training. This gives us an unbiased estimate of real-world performance.\n",
    "\n",
    "**Testing Process:**\n",
    "\n",
    "1. Load 5 samples from `validation.csv`\n",
    "2. Separate **features** (columns 1+) from **labels** (column 0)\n",
    "3. Send features to the endpoint as CSV\n",
    "4. Compare predictions to ground truth labels\n",
    "\n",
    "### Understanding the Output\n",
    "\n",
    "The model returns a **probability score** between 0.0 and 1.0:\n",
    "\n",
    "| Score Range | Interpretation | Action |\n",
    "|-------------|----------------|--------|\n",
    "| 0.0 - 0.3 | High confidence: Normal | âœ… Allow traffic |\n",
    "| 0.3 - 0.7 | Uncertain | âš ï¸ Flag for review |\n",
    "| 0.7 - 1.0 | High confidence: Attack | ðŸš¨ Block & alert |\n",
    "\n",
    "### SNS Alerting Integration\n",
    "\n",
    "When the model detects a high-confidence threat (score > 0.90), we trigger an **AWS SNS alert** to notify security teams via email, SMS, or Slack webhook.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f16f7-81b8-4e59-b0ba-cc7cb6017467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the endpoint with a sample attack...\n",
      "\n",
      "[DEBUG] Raw API Response:\n",
      "0.9998563528060913\n",
      "0.9997653365135193\n",
      "0.999882698059082\n",
      "0.00025598349748179317\n",
      "0.9998220801353455\n",
      "\n",
      "--- ðŸ›¡ï¸ SAGEWALL DETECTION LOG ---\n",
      "Packet #1: Real=ATTACK | AI Confidence=0.9999 -> âœ… CAUGHT\n",
      "Packet #2: Real=ATTACK | AI Confidence=0.9998 -> âœ… CAUGHT\n",
      "Packet #3: Real=ATTACK | AI Confidence=0.9999 -> âœ… CAUGHT\n",
      "Packet #4: Real=Normal | AI Confidence=0.0003 -> âœ… CLEARED\n",
      "Packet #5: Real=ATTACK | AI Confidence=0.9998 -> âœ… CAUGHT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from alerts import send_alert\n",
    "\n",
    "# 6. Test the Endpoint (FIXED VERSION)\n",
    "# ------------------------------------\n",
    "print(\"Testing the endpoint with a sample attack...\")\n",
    "\n",
    "# Configure your SNS Topic ARN for alerts (replace with your actual ARN)\n",
    "SNS_TOPIC_ARN = 'arn:aws:sns:us-east-1:015283627438:sagewall-alerts'\n",
    "\n",
    "# Grab the first 5 rows from our validation set\n",
    "test_data = pd.read_csv('validation.csv', header=None)\n",
    "\n",
    "# Separate answers (Col 0) and questions (Col 1+)\n",
    "test_labels = test_data.iloc[:5, 0] \n",
    "test_samples = test_data.iloc[:5, 1:]\n",
    "\n",
    "# Ask the AI for predictions\n",
    "payload = test_samples.to_csv(header=False, index=False)\n",
    "response = xgb_predictor.predict(payload).decode('utf-8')\n",
    "\n",
    "# --- DEBUG: Show me exactly what the AI said ---\n",
    "print(f\"\\n[DEBUG] Raw API Response:\\n{response.strip()}\") \n",
    "# -----------------------------------------------\n",
    "\n",
    "# FIX: Replace newlines with commas so NumPy can read it\n",
    "predictions = np.fromstring(response.replace('\\n', ','), sep=',')\n",
    "\n",
    "print(\"\\n--- ðŸ›¡ï¸ SAGEWALL DETECTION LOG ---\")\n",
    "for i in range(5):\n",
    "    truth = \"ATTACK\" if test_labels.iloc[i] == 1 else \"Normal\"\n",
    "    pred_score = predictions[i]\n",
    "    ai_guess = \"ATTACK\" if pred_score > 0.5 else \"Normal\"\n",
    "    \n",
    "    if truth == ai_guess:\n",
    "        status = \"âœ… CAUGHT\" if truth == \"ATTACK\" else \"âœ… CLEARED\"\n",
    "    else:\n",
    "        status = \"âŒ MISSED\"\n",
    "    \n",
    "    print(f\"Packet #{i+1}: Real={truth} | AI Confidence={pred_score:.4f} -> {status}\")\n",
    "    \n",
    "    # Send SNS alert for high-confidence threats (score > 0.90)\n",
    "    send_alert(pred_score, SNS_TOPIC_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d328e",
   "metadata": {},
   "source": [
    "## 4. Resource Cleanup\n",
    "\n",
    "### âš ï¸ CRITICAL: Delete the Endpoint to Stop Billing\n",
    "\n",
    "SageMaker endpoints are **billed by the hour** whether you're using them or not. A `ml.t2.medium` instance costs approximately **$0.05/hour** (~$36/month if left running).\n",
    "\n",
    "**Always run this cell when you're done testing!**\n",
    "\n",
    "The `delete_endpoint()` method:\n",
    "1. Terminates the EC2 instance hosting the model\n",
    "2. Deletes the endpoint configuration\n",
    "3. Stops all billing for this resource\n",
    "\n",
    "> ðŸ’¡ **Note:** The trained model artifact remains in S3 â€” you can redeploy anytime by running Cell 1 again (after retraining or loading the estimator).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ‰ Summary\n",
    "\n",
    "You've successfully:\n",
    "\n",
    "- [x] Configured AWS SageMaker environment\n",
    "- [x] Split data into training/validation sets\n",
    "- [x] Trained an XGBoost binary classifier\n",
    "- [x] Deployed a real-time inference endpoint\n",
    "- [x] Tested predictions on held-out data\n",
    "- [x] Cleaned up resources to avoid charges\n",
    "\n",
    "**Next Steps:**\n",
    "- Use the Streamlit app (`app.py`) for interactive testing\n",
    "- Integrate with Lambda for automated batch processing\n",
    "- Set up CloudWatch alarms for endpoint monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c76d5900-83f5-43f5-8c2a-9a2dedfe519a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: sagemaker-xgboost-2025-12-21-17-42-13-588\n",
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-xgboost-2025-12-21-17-42-13-588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# 7. CLEANUP\n",
    "# ----------\n",
    "xgb_predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f714dac-7059-4b7c-917a-792f81f259dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
