{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abe94da",
   "metadata": {},
   "source": [
    "# SageWall: XGBoost Training Pipeline\n",
    "\n",
    "**Author:** Andrew Zheng | **Date:** December 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Read the DevLog!\n",
    "\n",
    "Please visit the [DevLog](devlog.md) and read my fun and clear explanations for my design choices!\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook trains and deploys a **binary classification model** to detect network intrusions in real-time. We use **XGBoost** (eXtreme Gradient Boosting) on AWS SageMaker to analyze network packet features and classify traffic as either **Normal** or **Attack**.\n",
    "\n",
    "### Contents\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| **1. Setup** | Initialize AWS SDKs and configure IAM permissions |\n",
    "| **2. Data Preparation** | Split dataset into training (80%) and validation (20%) |\n",
    "| **3. Model Training** | Configure XGBoost hyperparameters and train on SageMaker |\n",
    "| **4. Deployment** | Provision a real-time inference endpoint |\n",
    "| **5. Testing** | Validate model accuracy on held-out samples |\n",
    "| **6. Cleanup** | Delete resources to stop billing |\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- AWS credentials configured (`aws configure`)\n",
    "- Processed data in S3 bucket (`sagewall-processed-zheng-1b`)\n",
    "- SageMaker execution role with appropriate permissions\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa95ec",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Data Preparation\n",
    "\n",
    "### 1.1 Initializing AWS SDKs\n",
    "\n",
    "We begin by importing the necessary libraries and establishing connections to AWS services:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|---------|\n",
    "| `sagemaker` | High-level SDK for ML training and deployment |\n",
    "| `boto3` | Low-level AWS SDK for S3 operations |\n",
    "| `pandas` | Data manipulation and CSV handling |\n",
    "\n",
    "**IAM Role:** SageMaker requires an execution role with permissions to:\n",
    "- Read/write to S3 buckets\n",
    "- Create training jobs and endpoints\n",
    "- Write logs to CloudWatch\n",
    "\n",
    "The `get_execution_role()` function automatically retrieves this role when running in SageMaker Studio.\n",
    "\n",
    "### 1.2 The 80/20 Split Strategy\n",
    "\n",
    "Machine learning models need two datasets:\n",
    "\n",
    "1. **Training Set (80%)**: The model learns patterns from this data\n",
    "2. **Validation Set (20%)**: We test accuracy on data the model has *never seen*\n",
    "\n",
    "> **Why split?** If we test on training data, we get falsely high accuracy. The model could simply \"memorize\" answers instead of learning patterns.\n",
    "\n",
    "We use `random_state=42` to ensure reproducibility.\n",
    "\n",
    "### 1.3 XGBoost Configuration\n",
    "\n",
    "**Why XGBoost?** It's consistently the top-performing algorithm on tabular data competitions. Key hyperparameters:\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|-----------|-------|-------------|\n",
    "| `max_depth` | 5 | Tree depth prevents overfitting (too complex = memorization) |\n",
    "| `eta` | 0.2 | Learning rate smaller = more cautious learning |\n",
    "| `gamma` | 4 | Minimum loss reduction regularization to prevent overfitting |\n",
    "| `min_child_weight` | 6 | Minimum samples per leaf avoids learning from noise |\n",
    "| `subsample` | 0.8 | Use 80% of data per tree reduces variance |\n",
    "| `objective` | `binary:logistic` | Binary classification (Attack vs Normal) |\n",
    "| `num_round` | 50 | Number of boosting rounds (trees to build) |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd8713-3ec7-4819-bdd9-154b8dca9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# 1. Setup Environment\n",
    "# -------------------\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = session.boto_region_name\n",
    "bucket_processed = 'sagewall-processed-zheng-1b'  # Your processed bucket\n",
    "prefix = 'model-input'\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "# 2. Prepare Data (Split into Train/Validation)\n",
    "# ---------------------------------------------\n",
    "print(\"\\nDownloading data from S3...\")\n",
    "s3 = boto3.client('s3')\n",
    "# Download the clean file we made in Phase 2\n",
    "s3.download_file(bucket_processed, 'KDDTrain+.txt', 'clean_data.csv')\n",
    "\n",
    "# Read with Pandas (No headers, as per our Lambda format)\n",
    "df = pd.read_csv('clean_data.csv', header=None)\n",
    "\n",
    "# Split: 80% for Training, 20% for Validation (The \"Exam\")\n",
    "train_data = df.sample(frac=0.8, random_state=42)\n",
    "val_data = df.drop(train_data.index)\n",
    "\n",
    "# Save locally without headers (SageMaker requirement)\n",
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "val_data.to_csv('validation.csv', index=False, header=False)\n",
    "\n",
    "# Upload the split files back to S3\n",
    "print(\"Uploading split datasets to S3...\")\n",
    "session.upload_data('train.csv', bucket=bucket_processed, key_prefix=prefix)\n",
    "session.upload_data('validation.csv', bucket=bucket_processed, key_prefix=prefix)\n",
    "\n",
    "s3_train_input = TrainingInput(s3_data=f's3://{bucket_processed}/{prefix}/train.csv', content_type='csv')\n",
    "s3_val_input = TrainingInput(s3_data=f's3://{bucket_processed}/{prefix}/validation.csv', content_type='csv')\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "# 3. Define the XGBoost Model\n",
    "# ---------------------------\n",
    "# We retrieve the official XGBoost Docker image from AWS\n",
    "container = image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "\n",
    "# Create the Estimator (The \"Teacher\")\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',  # Standard training instance\n",
    "    output_path=f's3://{bucket_processed}/model-output/',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Set Hyperparameters (The \"Teaching Style\")\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    objective='binary:logistic',  # \"Is it an Attack? Yes/No\"\n",
    "    num_round=50\n",
    ")\n",
    "\n",
    "# 4. Start Training\n",
    "# -----------------\n",
    "print(\"\\nStarting Training Job... (This takes ~3-5 minutes)\")\n",
    "xgb_estimator.fit({'train': s3_train_input, 'validation': s3_val_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93761f16",
   "metadata": {},
   "source": [
    "## 2. Model Deployment\n",
    "\n",
    "### Creating a Real-Time Inference Endpoint\n",
    "\n",
    "After training completes, the model artifact (a serialized XGBoost file) is saved to S3. To use it for predictions, we must **deploy** it to a live endpoint.\n",
    "\n",
    "**What happens during deployment:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Model Artifact â”‚  â†’   â”‚  Docker         â”‚  â†’   â”‚  EC2 Instance   â”‚\n",
    "â”‚  (S3 .tar.gz)   â”‚      â”‚  Container      â”‚      â”‚  (ml.t2.medium) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "1. SageMaker downloads the model from S3\n",
    "2. Loads it into a pre-built XGBoost Docker container\n",
    "3. Provisions an EC2 instance to host the API\n",
    "4. Exposes an HTTPS endpoint for real-time predictions\n",
    "\n",
    "### Instance Selection\n",
    "\n",
    "| Instance | vCPU | Memory | Cost/Hour | Use Case |\n",
    "|----------|------|--------|-----------|----------|\n",
    "| `ml.t2.medium` | 2 | 4 GB | ~$0.05 | Development/Testing âœ“ |\n",
    "| `ml.m5.large` | 2 | 8 GB | ~$0.12 | Light Production |\n",
    "| `ml.c5.xlarge` | 4 | 8 GB | ~$0.20 | High Throughput |\n",
    "\n",
    "We use `ml.t2.medium` for cost-effective testing. For production workloads with high request volume, scale up to `ml.m5` or `ml.c5` instances.\n",
    "\n",
    "> **Deployment takes 5-8 minutes** â€” SageMaker must provision infrastructure and health-check the endpoint.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388b7ee-595f-4cb2-b417-c5015bfe2822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Deploy the Model\n",
    "# -------------------\n",
    "print(\"Deploying model to a real-time endpoint... (This takes 5-8 minutes)\")\n",
    "\n",
    "# We create a \"Predictor\" (The Interface)\n",
    "xgb_predictor = xgb_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',  # Cost-effective instance for inference\n",
    "    serializer=sagemaker.serializers.CSVSerializer() # We will send it CSV data\n",
    ")\n",
    "\n",
    "print(f\"\\nModel deployed! Endpoint name: {xgb_predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe3f9f",
   "metadata": {},
   "source": [
    "## 3. Model Testing & Validation\n",
    "\n",
    "### Evaluating on Held-Out Data\n",
    "\n",
    "Now we test the deployed model on our **validation set** â€” data the model has never seen during training. This gives us an unbiased estimate of real-world performance.\n",
    "\n",
    "**Testing Process:**\n",
    "\n",
    "1. Load 5 samples from `validation.csv`\n",
    "2. Separate **features** (columns 1+) from **labels** (column 0)\n",
    "3. Send features to the endpoint as CSV\n",
    "4. Compare predictions to ground truth labels\n",
    "\n",
    "### Understanding the Output\n",
    "\n",
    "The model returns a **probability score** between 0.0 and 1.0:\n",
    "\n",
    "| Score Range | Interpretation | Action |\n",
    "|-------------|----------------|--------|\n",
    "| 0.0 - 0.3 | High confidence: Normal | âœ… Allow traffic |\n",
    "| 0.3 - 0.7 | Uncertain | âš ï¸ Flag for review |\n",
    "| 0.7 - 1.0 | High confidence: Attack | ðŸš¨ Block & alert |\n",
    "\n",
    "### SNS Alerting Integration\n",
    "\n",
    "When the model detects a high-confidence threat (score > 0.90), we trigger an **AWS SNS alert** to notify security teams via email, SMS, or Slack webhook.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f16f7-81b8-4e59-b0ba-cc7cb6017467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from alerts import send_alert\n",
    "\n",
    "# 6. Test the Endpoint (FIXED VERSION)\n",
    "# ------------------------------------\n",
    "print(\"Testing the endpoint with a sample attack...\")\n",
    "\n",
    "# Configure your SNS Topic ARN for alerts (replace with your actual ARN)\n",
    "SNS_TOPIC_ARN = 'arn:aws:sns:us-east-1:015283627438:sagewall-alerts'\n",
    "\n",
    "# Grab the first 5 rows from our validation set\n",
    "test_data = pd.read_csv('validation.csv', header=None)\n",
    "\n",
    "# Separate answers (Col 0) and questions (Col 1+)\n",
    "test_labels = test_data.iloc[:5, 0] \n",
    "test_samples = test_data.iloc[:5, 1:]\n",
    "\n",
    "# Ask the AI for predictions\n",
    "payload = test_samples.to_csv(header=False, index=False)\n",
    "response = xgb_predictor.predict(payload).decode('utf-8')\n",
    "\n",
    "# --- DEBUG: Show me exactly what the AI said ---\n",
    "print(f\"\\n[DEBUG] Raw API Response:\\n{response.strip()}\") \n",
    "# -----------------------------------------------\n",
    "\n",
    "# FIX: Replace newlines with commas so NumPy can read it\n",
    "predictions = np.fromstring(response.replace('\\n', ','), sep=',')\n",
    "\n",
    "print(\"\\n--- ðŸ›¡ï¸ SAGEWALL DETECTION LOG ---\")\n",
    "for i in range(5):\n",
    "    truth = \"ATTACK\" if test_labels.iloc[i] == 1 else \"Normal\"\n",
    "    pred_score = predictions[i]\n",
    "    ai_guess = \"ATTACK\" if pred_score > 0.5 else \"Normal\"\n",
    "    \n",
    "    if truth == ai_guess:\n",
    "        status = \"âœ… CAUGHT\" if truth == \"ATTACK\" else \"âœ… CLEARED\"\n",
    "    else:\n",
    "        status = \"âŒ MISSED\"\n",
    "    \n",
    "    print(f\"Packet #{i+1}: Real={truth} | AI Confidence={pred_score:.4f} -> {status}\")\n",
    "    \n",
    "    # Send SNS alert for high-confidence threats (score > 0.90)\n",
    "    send_alert(pred_score, SNS_TOPIC_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d328e",
   "metadata": {},
   "source": [
    "## 4. Resource Cleanup\n",
    "\n",
    "### Delete the Endpoint to Stop Billing\n",
    "\n",
    "SageMaker endpoints are **billed by the hour** whether you're using them or not. A `ml.t2.medium` instance costs approximately **$0.05/hour** (~$36/month if left running).\n",
    "\n",
    "**Always run this cell when you're done testing!**\n",
    "\n",
    "The `delete_endpoint()` method:\n",
    "1. Terminates the EC2 instance hosting the model\n",
    "2. Deletes the endpoint configuration\n",
    "3. Stops all billing for this resource\n",
    "\n",
    "> **Note:** The trained model artifact remains in S3 â€” you can redeploy anytime by running Cell 1 again (after retraining or loading the estimator).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've successfully:\n",
    "\n",
    "- [x] Configured AWS SageMaker environment\n",
    "- [x] Split data into training/validation sets\n",
    "- [x] Trained an XGBoost binary classifier\n",
    "- [x] Deployed a real-time inference endpoint\n",
    "- [x] Tested predictions on held-out data\n",
    "- [x] Cleaned up resources to avoid charges\n",
    "\n",
    "**Next Steps:**\n",
    "- Use the Streamlit app (`app.py`) for interactive testing\n",
    "- Integrate with Lambda for automated batch processing\n",
    "- Set up CloudWatch alarms for endpoint monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d5900-83f5-43f5-8c2a-9a2dedfe519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. CLEANUP\n",
    "# ----------\n",
    "xgb_predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
